{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-10T18:48:26.640071Z","iopub.execute_input":"2022-11-10T18:48:26.640635Z","iopub.status.idle":"2022-11-10T18:48:26.684489Z","shell.execute_reply.started":"2022-11-10T18:48:26.640528Z","shell.execute_reply":"2022-11-10T18:48:26.683319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv","metadata":{}},{"cell_type":"markdown","source":"READING THE DATA IN THE CSV FILE ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/spam-mails-dataset/spam_ham_dataset.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:26.853764Z","iopub.execute_input":"2022-11-10T18:48:26.854517Z","iopub.status.idle":"2022-11-10T18:48:26.934459Z","shell.execute_reply.started":"2022-11-10T18:48:26.85448Z","shell.execute_reply":"2022-11-10T18:48:26.932921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/spam-mails-dataset/spam_ham_dataset.csv').dropna()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:27.112871Z","iopub.execute_input":"2022-11-10T18:48:27.113316Z","iopub.status.idle":"2022-11-10T18:48:27.192152Z","shell.execute_reply.started":"2022-11-10T18:48:27.113281Z","shell.execute_reply":"2022-11-10T18:48:27.190916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMPORTING THE REQUIRED LIBRARIES ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport nltk\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport random\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:27.47433Z","iopub.execute_input":"2022-11-10T18:48:27.474825Z","iopub.status.idle":"2022-11-10T18:48:47.516004Z","shell.execute_reply.started":"2022-11-10T18:48:27.474779Z","shell.execute_reply":"2022-11-10T18:48:47.514679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!","metadata":{}},{"cell_type":"markdown","source":"USING BINARY NAIVE BAYE'S THEOREM TO CREATE A SPAM AND HAM FILTER AND MAKING THE DATA CLASSIFIED AND CALCULATING THE CONFUSION MATRIX IN WHICH THE TRUE POSITIVE , TRUE NEGATIVE , FALSE POSITIVE , FALSE NEGATIVE VALUES ARE OBTAINED THROUGH WHICH CALCULATING THE ACCURACY , SPECIFICITY , SENSITIVITY VALUES THE  dATAFRAME  WILL  BE SPLIT INTO  TRAIN  AND TEST. THE TRAINING DATASET WILL BE USED TO BUILD THE MODEL","metadata":{}},{"cell_type":"code","source":"class BinaryNaiveBayes():\n        \n    def vectorize(self, data, splitRatio):\n        arr = data['text'].to_list()\n        y = data['label_num'].to_list()\n        \n        temp = list(zip(arr, y))\n        random.shuffle(temp)\n        arr, y = zip(*temp)\n        \n        x = self.vectorizer.fit_transform(arr).toarray()\n        train_x = x[:int(len(x)*splitRatio)]\n        test_x = x[int(len(x)*splitRatio):]\n        \n        train_y = y[:int(len(y)*splitRatio)]\n        test_y = y[int(len(y)*splitRatio):]\n        return train_x, train_y, test_x, test_y\n    \n    def count(self):\n        for i in range(len(self.labels)):\n            currX = self.data[i]\n            label = self.labels[i]\n            if(label == 0):\n                for i in range(len(currX)):\n                    self.positiveHashMap[i] += currX[i]\n                self.numPositives += 1\n            else:\n                for i in range(len(currX)):\n                    self.negativeHashMap[i] += currX[i]\n                self.numNegatives += 1\n    \n    def __init__(self, data):\n        self.numFeatures = 20000\n        self.vectorizer = CountVectorizer(max_features=self.numFeatures)\n        self.data, self.labels, self.test_data, self.test_labels = self.vectorize(data, 0.8) \n        self.positiveHashMap = [0 for i in range(self.numFeatures)]\n        self.numPositives = 0\n        self.negativeHashMap = [0 for i in range(self.numFeatures)]\n        self.numNegatives = 0\n        self.test_matrix = [[0,0],[0,0]]\n        \n        self.count()\n        self.posSum = sum(self.positiveHashMap)\n        self.negSum = sum(self.negativeHashMap)\n        self.lemma = WordNetLemmatizer()\n    \n    def predict(self, document):\n        \"\"\"print(self.numPositives, self.numNegatives)\n        print(self.posSum, self.negSum)\n        print(self.positiveHashMap)\n        print(self.negativeHashMap)\"\"\"\n        priorPos = self.numPositives/(self.numPositives+self.numNegatives)\n        likelihoodPos = math.log(priorPos)\n        priorNeg = self.numNegatives/(self.numPositives+self.numNegatives)\n        likelihoodNeg = math.log(priorNeg)\n        for i in range(self.numFeatures):\n            if document[i] == 0:\n                continue\n            #print(likelihoodPos, likelihoodNeg)\n            likelihoodPos += math.log(self.prob(i, 0))\n            likelihoodNeg += math.log(self.prob(i, 1))\n            \n        #print(math.exp(likelihoodPos), math.exp(likelihoodNeg))\n        if(likelihoodPos > likelihoodNeg):\n            return 0\n        return 1\n    \n    def prob(self, i, c):\n        if(c == 0):\n            return (self.positiveHashMap[i]+1)/(self.posSum + self.numFeatures)\n        return (self.negativeHashMap[i]+1)/(self.negSum + self.numFeatures)\n    \n    def test(self):\n        for i in range(len(self.test_data)):\n            prediction = self.predict(self.test_data[i])\n            label = self.test_labels[i]\n            self.test_matrix[prediction][label] += 1\n      \n    def Sensitivity(self):\n        return self.test_matrix[1][1]/(self.test_matrix[1][1] + self.test_matrix[1][0])\n    \n    def Specificity(self):\n        return self.test_matrix[0][0]/(self.test_matrix[0][0] + self.test_matrix[0][1])\n    \n    def accuracy(self):\n        return (self.test_matrix[0][0]+self.test_matrix[1][1])/(sum(self.test_matrix[0]) + sum(self.test_matrix[1]))\n    \n    def text_to_vector(self, text):\n        text = re.sub(r\"http\\S+\", \"\", text)\n        pattern = \"[^a-zA-Z0-9]\"\n        text = re.sub(pattern,\" \",text)\n        text = text.lower()\n        text = nltk.word_tokenize(text)\n        text = [self.lemma.lemmatize(word) for word in text]\n        stopwords = nltk.corpus.stopwords.words(\"english\")\n        text = [word for word in text if word not in stopwords]\n        text = \" \".join(text)\n        return self.vectorizer.transform([text]).toarray()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:47.518132Z","iopub.execute_input":"2022-11-10T18:48:47.518995Z","iopub.status.idle":"2022-11-10T18:48:47.546146Z","shell.execute_reply.started":"2022-11-10T18:48:47.51896Z","shell.execute_reply":"2022-11-10T18:48:47.544606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE ACCURACY","metadata":{}},{"cell_type":"code","source":"nb = BinaryNaiveBayes(data)\nnb.test()\nnb.accuracy()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:47.548043Z","iopub.execute_input":"2022-11-10T18:48:47.548504Z","iopub.status.idle":"2022-11-10T18:49:16.311415Z","shell.execute_reply.started":"2022-11-10T18:48:47.548465Z","shell.execute_reply":"2022-11-10T18:49:16.309875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb.test_matrix","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:49:16.314484Z","iopub.execute_input":"2022-11-10T18:49:16.314902Z","iopub.status.idle":"2022-11-10T18:49:16.321899Z","shell.execute_reply.started":"2022-11-10T18:49:16.31487Z","shell.execute_reply":"2022-11-10T18:49:16.320929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SPECIFICITY","metadata":{}},{"cell_type":"code","source":"nb = BinaryNaiveBayes(data)\nnb.test()\nnb.Specificity()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:49:16.323202Z","iopub.execute_input":"2022-11-10T18:49:16.323799Z","iopub.status.idle":"2022-11-10T18:49:44.479395Z","shell.execute_reply.started":"2022-11-10T18:49:16.323752Z","shell.execute_reply":"2022-11-10T18:49:44.478194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SENSITIVITY","metadata":{}},{"cell_type":"code","source":"nb = BinaryNaiveBayes(data)\nnb.test()\nnb.Sensitivity()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:58:34.058076Z","iopub.execute_input":"2022-11-10T18:58:34.058501Z","iopub.status.idle":"2022-11-10T18:59:01.987604Z","shell.execute_reply.started":"2022-11-10T18:58:34.058464Z","shell.execute_reply":"2022-11-10T18:59:01.986741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"THE CORRECT CLASSIFICATION AND MISCLASSIFICATION ARE DETERMINED BY THE CONFUSION MATRIX\nCREATING A FUNCTION FOR CONFUSION MATRIX AND THE LAYOUT OF THE CONFUSION MATRIX AND THE COLOURS THAT ARE GIVE TO THE VALUES OF TN,TP,FN,FP AND SETTING THE LABELS AND TITLE TO THE MATRIX","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http://matplotlib.org/examples/color/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) / float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            Sensitivity = cf[1,1] / (cf[1,1]+cf[1,0])\n            Specificity    = cf[0,0] / (cf[0,0]+cf[0,1])\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nSensitivity={:0.3f}\\nSpecificity={:0.3f}\".format(\n                accuracy,Sensitivity,Specificity)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:58:04.040161Z","iopub.execute_input":"2022-11-10T18:58:04.041234Z","iopub.status.idle":"2022-11-10T18:58:04.059474Z","shell.execute_reply.started":"2022-11-10T18:58:04.041191Z","shell.execute_reply":"2022-11-10T18:58:04.058169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PRINTING THE CONFUSION MATRIX","metadata":{}},{"cell_type":"code","source":"make_confusion_matrix(np.array(nb.test_matrix), categories = ['Not Spam', 'Spam'], title = 'Confusion Matrix for Naive Bayes Spam Prediction Algorithm')","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:50.308033Z","iopub.execute_input":"2022-11-10T18:57:50.308501Z","iopub.status.idle":"2022-11-10T18:57:50.605777Z","shell.execute_reply.started":"2022-11-10T18:57:50.308466Z","shell.execute_reply":"2022-11-10T18:57:50.604565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PRINTING THE TOTAL VALUES OF SPAM AND HAM IN THE GIVEN DATASET TO THE IN-DEPTH IDEA OF OUR GIVEN DATASET","metadata":{}},{"cell_type":"code","source":"data.label_num.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:46.824452Z","iopub.execute_input":"2022-11-10T18:57:46.824918Z","iopub.status.idle":"2022-11-10T18:57:46.833182Z","shell.execute_reply.started":"2022-11-10T18:57:46.824876Z","shell.execute_reply":"2022-11-10T18:57:46.832242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FINDING THE LENGTH OF EVERY SPAM OR HAM MESSAGE TO HELP CATEGORIZE THEM BETTER","metadata":{}},{"cell_type":"code","source":"data['length'] = data['text'].apply(len)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:43.444933Z","iopub.execute_input":"2022-11-10T18:57:43.445375Z","iopub.status.idle":"2022-11-10T18:57:43.463966Z","shell.execute_reply.started":"2022-11-10T18:57:43.445345Z","shell.execute_reply":"2022-11-10T18:57:43.46272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRAINING OUR MODEL USING THE SKLEARN LIBRARY","metadata":{}},{"cell_type":"code","source":"X = data['text']\nY = data['label_num']\nfrom sklearn.model_selection import train_test_split as tt\nX_train, X_test, Y_train, Y_test = tt(X, Y,test_size=0.2, random_state=100)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:34.144524Z","iopub.execute_input":"2022-11-10T18:57:34.144993Z","iopub.status.idle":"2022-11-10T18:57:34.15461Z","shell.execute_reply.started":"2022-11-10T18:57:34.144961Z","shell.execute_reply":"2022-11-10T18:57:34.152979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FINDING THE XTEST AND TRAIN SHAPE FOR THE SPLIT DATA","metadata":{}},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:30.003304Z","iopub.execute_input":"2022-11-10T18:57:30.003782Z","iopub.status.idle":"2022-11-10T18:57:30.011346Z","shell.execute_reply.started":"2022-11-10T18:57:30.003746Z","shell.execute_reply":"2022-11-10T18:57:30.009918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:25.993988Z","iopub.execute_input":"2022-11-10T18:57:25.994534Z","iopub.status.idle":"2022-11-10T18:57:26.004133Z","shell.execute_reply.started":"2022-11-10T18:57:25.994485Z","shell.execute_reply":"2022-11-10T18:57:26.002553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WORDS THAT ARE NOT IN OUR DICTIONARY ARE EXCLUDED FROM OUR DATA AS THEY CAUSE UNNECESSARY ERRORS.FOR THIS , WE USE SKLEARN FEATURE","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvector = CountVectorizer(stop_words ='english')\nvector.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:12.945454Z","iopub.execute_input":"2022-11-10T18:57:12.945962Z","iopub.status.idle":"2022-11-10T18:57:13.656658Z","shell.execute_reply.started":"2022-11-10T18:57:12.945921Z","shell.execute_reply":"2022-11-10T18:57:13.655403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PRINTING THE FREQUENCY OF WORDS THAT ARE BEING USED","metadata":{}},{"cell_type":"code","source":"vector.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:57:19.792446Z","iopub.execute_input":"2022-11-10T18:57:19.792866Z","iopub.status.idle":"2022-11-10T18:57:19.832555Z","shell.execute_reply.started":"2022-11-10T18:57:19.792833Z","shell.execute_reply":"2022-11-10T18:57:19.831229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRAINING THE MODEL WITH THE FINAL VECTOR INPUTS AND PROCEED WITH THE PREDICTIONS","metadata":{}},{"cell_type":"code","source":"X_train_transformed =vector.transform(X_train)\nX_test_transformed =vector.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:48.082629Z","iopub.execute_input":"2022-11-10T18:56:48.083627Z","iopub.status.idle":"2022-11-10T18:56:48.797018Z","shell.execute_reply.started":"2022-11-10T18:56:48.083583Z","shell.execute_reply":"2022-11-10T18:56:48.79553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BULDING THE FINAL MODEL USING THE MULTINOMIAL NAIVE BAYES LIBRARIES FROM SKLEARN","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_transformed,Y_train)\ny_pred = model.predict(X_test_transformed)\ny_pred_prob = model.predict_proba(X_test_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:44.947401Z","iopub.execute_input":"2022-11-10T18:56:44.948587Z","iopub.status.idle":"2022-11-10T18:56:44.964327Z","shell.execute_reply.started":"2022-11-10T18:56:44.948546Z","shell.execute_reply":"2022-11-10T18:56:44.963096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PERFORMING THE PREDICTIONS - TO HELP THE MODEL PREDICT,WE IMPORT THE CONFUSION MATRIX,ACCURACY SCORE","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(Y_test,y_pred))\nprint()\nprint(accuracy_score(Y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:30.70157Z","iopub.execute_input":"2022-11-10T18:56:30.701989Z","iopub.status.idle":"2022-11-10T18:56:30.711113Z","shell.execute_reply.started":"2022-11-10T18:56:30.701959Z","shell.execute_reply":"2022-11-10T18:56:30.709726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PRINTING THE CONFUSION MATRIX USING MULLTIBIONOMIAL NAIVES BAYES THEOREM","metadata":{}},{"cell_type":"code","source":" from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nconfusion_matrix = metrics.confusion_matrix(Y_test,y_pred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\ncm_display.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:33.393839Z","iopub.execute_input":"2022-11-10T18:56:33.394617Z","iopub.status.idle":"2022-11-10T18:56:33.623689Z","shell.execute_reply.started":"2022-11-10T18:56:33.394568Z","shell.execute_reply":"2022-11-10T18:56:33.622474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(Y_test,y_pred)\ncm","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:59.152488Z","iopub.execute_input":"2022-11-10T18:56:59.152984Z","iopub.status.idle":"2022-11-10T18:56:59.174009Z","shell.execute_reply.started":"2022-11-10T18:56:59.152946Z","shell.execute_reply":"2022-11-10T18:56:59.172184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SENSITIVITY FROM THE CONFUSION MATRIX","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_true = [0, 0, 0, 1, 1, 1, 1, 1]\ny_pred = [0, 1, 0, 1, 0, 1, 0, 1]\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\nsensitivity= tp / (fn+tp)\nsensitivity","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:06.397252Z","iopub.execute_input":"2022-11-10T18:56:06.397732Z","iopub.status.idle":"2022-11-10T18:56:06.409219Z","shell.execute_reply.started":"2022-11-10T18:56:06.397694Z","shell.execute_reply":"2022-11-10T18:56:06.407793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SPECIFICITY FROM THE CONFUSION MATRIX","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_true = [0, 0, 0, 1, 1, 1, 1, 1]\ny_pred = [0, 1, 0, 1, 0, 1, 0, 1]\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\nspecificity= tn / (tn+fp)\nspecificity","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:56:02.58892Z","iopub.execute_input":"2022-11-10T18:56:02.58949Z","iopub.status.idle":"2022-11-10T18:56:02.601072Z","shell.execute_reply.started":"2022-11-10T18:56:02.589447Z","shell.execute_reply":"2022-11-10T18:56:02.599671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMPORTING THE ROC CURVE FEATURE FROM SKLEARN TO FIND TRUE AND FALSE POSITIVE RATES","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred_prob[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:58.936882Z","iopub.execute_input":"2022-11-10T18:55:58.938179Z","iopub.status.idle":"2022-11-10T18:55:58.946605Z","shell.execute_reply.started":"2022-11-10T18:55:58.938139Z","shell.execute_reply":"2022-11-10T18:55:58.945338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FROM THE CONFUSION MATRIX ACCESSING THE TP,FP,FN,TN VALUES","metadata":{}},{"cell_type":"code","source":"TP = cm[1][1]\nFP = cm[0][1]\nFN = cm[1][0]\nTN = cm[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:54.107972Z","iopub.execute_input":"2022-11-10T18:55:54.10877Z","iopub.status.idle":"2022-11-10T18:55:54.115866Z","shell.execute_reply.started":"2022-11-10T18:55:54.108723Z","shell.execute_reply":"2022-11-10T18:55:54.114448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PRINTING TPR,FPR,THRESHOLDS","metadata":{}},{"cell_type":"code","source":"print(false_positive_rate)\nprint()\nprint(true_positive_rate)\nprint()\nprint(thresholds)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:51.312747Z","iopub.execute_input":"2022-11-10T18:55:51.313168Z","iopub.status.idle":"2022-11-10T18:55:51.321708Z","shell.execute_reply.started":"2022-11-10T18:55:51.313137Z","shell.execute_reply":"2022-11-10T18:55:51.320349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ORGANISING THE TPR,FPR,THRESHOLD VALUES IN A DATAFRAME","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame({'Threshold': thresholds, \n              'TPR': true_positive_rate, \n              'FPR':false_positive_rate\n             })\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:48.497619Z","iopub.execute_input":"2022-11-10T18:55:48.498332Z","iopub.status.idle":"2022-11-10T18:55:48.520994Z","shell.execute_reply.started":"2022-11-10T18:55:48.498294Z","shell.execute_reply":"2022-11-10T18:55:48.51772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SPEACIFICITY THAT IS 1-FPR","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred_prob[:,1])\nspecificity = 1-false_positive_rate\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:45.827552Z","iopub.execute_input":"2022-11-10T18:55:45.827981Z","iopub.status.idle":"2022-11-10T18:55:45.840143Z","shell.execute_reply.started":"2022-11-10T18:55:45.827951Z","shell.execute_reply":"2022-11-10T18:55:45.838713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE SENSITITY THAT IS TPR","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred_prob[:,1])\nsensitivity = true_positive_rate\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:42.355054Z","iopub.execute_input":"2022-11-10T18:55:42.355559Z","iopub.status.idle":"2022-11-10T18:55:42.370155Z","shell.execute_reply.started":"2022-11-10T18:55:42.35552Z","shell.execute_reply":"2022-11-10T18:55:42.368808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLOTTING THE GRAPH AGAINST SENSITIVITY AND THRESHOLD","metadata":{}},{"cell_type":"code","source":"%matplotlib inline  \nplt.ylabel('sensitivity')     \nplt.xlabel('threshold')\nplt.title('ROC')\nplt.plot(thresholds, sensitivity)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:39.032867Z","iopub.execute_input":"2022-11-10T18:55:39.033369Z","iopub.status.idle":"2022-11-10T18:55:39.273391Z","shell.execute_reply.started":"2022-11-10T18:55:39.033332Z","shell.execute_reply":"2022-11-10T18:55:39.272165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred_prob[:,1])\nspecificity = 1-false_positive_rate\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:33.90623Z","iopub.execute_input":"2022-11-10T18:55:33.906741Z","iopub.status.idle":"2022-11-10T18:55:33.919323Z","shell.execute_reply.started":"2022-11-10T18:55:33.906705Z","shell.execute_reply":"2022-11-10T18:55:33.917831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLOTTING THE GARPH AGAINST SPECIFICITY AND THRESHOLD","metadata":{}},{"cell_type":"code","source":"%matplotlib inline  \nplt.ylabel('specificity')     \nplt.xlabel('threshold')\nplt.title('ROC')\nplt.plot(thresholds, specificity)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:30.830123Z","iopub.execute_input":"2022-11-10T18:55:30.830631Z","iopub.status.idle":"2022-11-10T18:55:31.070409Z","shell.execute_reply.started":"2022-11-10T18:55:30.830585Z","shell.execute_reply":"2022-11-10T18:55:31.068938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE PREVELENCE VALUE FROM THE CONFUSION MATRIX","metadata":{}},{"cell_type":"code","source":"prevelence = (FN+TP)/(FN+TP+FP+TN)\nprevelence","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:26.75154Z","iopub.execute_input":"2022-11-10T18:55:26.752005Z","iopub.status.idle":"2022-11-10T18:55:26.761259Z","shell.execute_reply.started":"2022-11-10T18:55:26.751968Z","shell.execute_reply":"2022-11-10T18:55:26.759857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE ACCURACY USING TPR,FPR AND PREVELENCE","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred_prob[:,1])\nspecificity = 1-false_positive_rate\nsensitivity = true_positive_rate\naccuracy = (true_positive_rate)*prevelence + specificity*(1-prevelence)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:22.866635Z","iopub.execute_input":"2022-11-10T18:55:22.86788Z","iopub.status.idle":"2022-11-10T18:55:22.879187Z","shell.execute_reply.started":"2022-11-10T18:55:22.867825Z","shell.execute_reply":"2022-11-10T18:55:22.878045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"POLTTING AGAINST ACCURACY AND THRESHOLD","metadata":{}},{"cell_type":"code","source":"%matplotlib inline  \nplt.ylabel('accuracy')     \nplt.xlabel('threshold')\nplt.title('ROC')\nplt.plot(thresholds, accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:19.870918Z","iopub.execute_input":"2022-11-10T18:55:19.871803Z","iopub.status.idle":"2022-11-10T18:55:20.120324Z","shell.execute_reply.started":"2022-11-10T18:55:19.871758Z","shell.execute_reply":"2022-11-10T18:55:20.119459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLOTTING SPECIFICITY,SENSITIVITY,ACCURACY AGAINST THRESHOLD","metadata":{}},{"cell_type":"code","source":"%matplotlib inline  \nplt.figure(figsize=(6,6),dpi=100)\nplt.ylabel('accuracy / specificity / sensitivity')\nplt.xlabel('Threshold')\nplt.title('ROC (accuracy vs threshold)')\nplt.plot(thresholds , accuracy , color = 'g', linestyle='-',label='accuracy')\nplt.plot(thresholds , specificity , color = 'r', linestyle='-',label='specificity')\nplt.plot(thresholds , sensitivity , color = 'b', linestyle='-',label='sensitivity')","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:15.310182Z","iopub.execute_input":"2022-11-10T18:55:15.310679Z","iopub.status.idle":"2022-11-10T18:55:15.598756Z","shell.execute_reply.started":"2022-11-10T18:55:15.310642Z","shell.execute_reply":"2022-11-10T18:55:15.597483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer=CountVectorizer()\nx=vectorizer.fit_transform(data['text'])\nprint(x[:2])","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:11.390567Z","iopub.execute_input":"2022-11-10T18:55:11.391026Z","iopub.status.idle":"2022-11-10T18:55:12.25729Z","shell.execute_reply.started":"2022-11-10T18:55:11.390991Z","shell.execute_reply":"2022-11-10T18:55:12.256069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as  np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\ny=data['label_num']\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33)\nprint(x_test)\nprint(x_train)\ndata['label_num']\nbayes=MultinomialNB()\nbayes.fit(x_train,y_train)\ny_predict=bayes.predict(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:06.496833Z","iopub.execute_input":"2022-11-10T18:55:06.497568Z","iopub.status.idle":"2022-11-10T18:55:06.526556Z","shell.execute_reply.started":"2022-11-10T18:55:06.497528Z","shell.execute_reply":"2022-11-10T18:55:06.524724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE MEAN OF CLASSIFIED DATA AND PLOTTING IT IN BARCHART FORM TO COMPARE IT WITH UNCLASSIFIED DATA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as  np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nmean=statistics.mean(data['label_num'])\nmode=statistics.mode(data['label_num'])\nmode=str(mode)\nprint(\"mean of the original data is: \",mean)\nprint(\"mode of the original data is: \",mode.replace('0','ham'))\nd1={\n    \"mean_of_classified_data\":\"mean\",\n    \"mean\":mean\n}\ndf2=pd.DataFrame(d1,index=[0])\ndf2\ndf2.plot(x=\"mean_of_classified_data\",y=\"mean\",kind=\"bar\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:55:01.445718Z","iopub.execute_input":"2022-11-10T18:55:01.446141Z","iopub.status.idle":"2022-11-10T18:55:01.655234Z","shell.execute_reply.started":"2022-11-10T18:55:01.446111Z","shell.execute_reply":"2022-11-10T18:55:01.653727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as  np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nmean=statistics.mean(data['label_num'])\nmode=statistics.mode(data['label_num'])\nmode=str(mode)\nwrongly_classified=[]\nfor x,y in zip(y_predict,y_test):\n  if x != y:\n    print(x,y)\n    wrongly_classified.append(x)\n   \nwrongly_classified","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:54:52.293808Z","iopub.execute_input":"2022-11-10T18:54:52.294287Z","iopub.status.idle":"2022-11-10T18:54:52.317713Z","shell.execute_reply.started":"2022-11-10T18:54:52.294252Z","shell.execute_reply":"2022-11-10T18:54:52.316381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE MEAN OF UNCLASSIFIED DATA AND PLOTTING IT IN BARCHART FORM TO COMPARE IT WITH CLASSIFIED DATA","metadata":{}},{"cell_type":"code","source":"s = np.sum(wrongly_classified)\nmean1 = s / (len(wrongly_classified))\nd={\n    \"mean_of_wrongly_classified_data\":\"mean\",\n    \"mean\":mean1\n}\ndf4=pd.DataFrame(d,index=[0])\ndf4.plot(x=\"mean_of_wrongly_classified_data\",y=\"mean\",kind=\"bar\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:54:48.239083Z","iopub.execute_input":"2022-11-10T18:54:48.239557Z","iopub.status.idle":"2022-11-10T18:54:48.442671Z","shell.execute_reply.started":"2022-11-10T18:54:48.239521Z","shell.execute_reply":"2022-11-10T18:54:48.441296Z"},"trusted":true},"execution_count":null,"outputs":[]}]}